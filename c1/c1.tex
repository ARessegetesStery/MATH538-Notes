\documentclass{article}
\usepackage{../header}

\begin{document}

\Makepagesectionhead{MATH 538}{Lie Algebra}{ARessegetes Stery}

\tableofcontents  
\clearpage

\section{Overview of Lie Algebras}

\begin{definition}[Lie Algebra]
    Let $\F$ be a field. A \textbf{Lie Algebra} is a vector space $L$ over $\F$ equipped with a bilinear map $[\cdot, \cdot]: L \times L \to L$ (the \textbf{Lie bracket}) satisfying the following properties:
    \begin{itemize}
        \item \emph{Alternating Property}: $[x, x] = 0$ for all $x \in L$. (For $\fchar{\F} \neq 2$, this is equivalent to \underline{antisymmetry}: $[x, y] = -[y, x]$ for all $x, y \in L$.)
        \item \emph{Jacobi Identity}: $[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0$ for all $x, y, z \in L$.
    \end{itemize}
    The \textbf{dimension} of a Lie algebra is the dimension of the underlying vector space.
\end{definition}
\nogap
\begin{remark}
    Throughout this note, for simplicity we will assume that $\fchar{\F} = 0$. Nevertheless, the limitations on field characteristics will be pointed out when it is clear that the result will fall in certain cases.
\end{remark}

\begin{example}
    Consider $V = \F^n$. Notice that $\dim_\F (\End(V)) = \dim_\F (\Mat_n(\F)) = n^2$ as vector spaces over $\F$; and they are further isomorphic. We can further show that they are isomorphic as Lie algebras. 
\end{example}

\begin{proposition}
    Define the Lie bracket on $\End(V)$ by $[f, g] = fg - gf$ (with the product the composition of functions). Then $\End(V)$ is a Lie Algebra.
\end{proposition}

\begin{proof}
    It suffices to verify that Lie bracket satisfies the alternating property and the Jacobi identity. 
    \begin{itemize}
        \item \emph{Alternating Property}: $[f, f] = ff - ff = 0$.
        \item \emph{Jacobi Identity}: 
        \begin{align*}
            [f, [g, h]] + [g, [h, f]] + [h, [f, g]] &= [f, gh - hg] + [g, hf - fh] + [h, fg - gf] \\
            &= f(gh - hg) - (gh - hg)f + g(hf - fh) - (hf - fh)g + h(fg - gf) - (fg - gf)h \\
            &= fgh - fgh - ghf + hgf + ghf - hfg - hfg + fhg + fhg - fgh - gfh + gfh \\
            &= 0.
        \end{align*}
    \end{itemize}
    Bi-linearity results directly from the linearity of functions.
\end{proof}

\begin{notation}
    The Lie algebra $(\End(V), [\cdot, \cdot])$ is denoted by $\gl(V)$.
\end{notation}
\nogap
\begin{example}
    Let $V = \R^n$. Then as vector spaces $\End(V) \simeq \Mat_n(\R) \simeq \gl(V)$ where $[A, B] = AB - BA$ for $A, B \in \Mat_n(\R)$.
\end{example}

The Lie Algebra $\End(V) \simeq \Mat_n(V) \simeq \gl(V)$ has a basis $\{ e_{ij} \}_{i, j = 1}^n$, where $\{e_{ij}\}$ represents the matrix with all zero entries except for $1$ in the $(i, j)$-th entry.

\begin{definition}[Lie Subalgebra]
    A \textbf{Lie Subalgebra} $K \subseteq L$ is a subspace of s.t. for all $x, y \in K$, $[x, y] \in K$.
\end{definition}
\nogap
\begin{definition}[Linear Lie Algebra]\label{def: linear lie algebra}
    Any subalgebra of $\gl(V)$ is called a \textbf{linear Lie algebra}.
\end{definition}

\begin{theorem}[Ado-Iwasawa]\label{thm:ado-iwasawa}
    Every finite dimensional Lie algebra is isomorphic to a linear Lie algebra.
\end{theorem}

\textstart
The proof of this statement requires more structure and will be deferred.

\TODO{search for proof}

\begin{example}[Classical Lie Algebra]\label{ex: classical Lie algebra}
    The following examples of Lie algebra constitute the \textbf{classical Lie algebra} which are the bulk of existing Lie algebras. As expected, they are closely related to matrices.
    \begin{enumerate}
        \item Let $V = \F^{n+1}$. The \textbf{special linear algebra} $A_n$ is
        \[
            A_n = \liealg{sl}_{n+1}(V) = \{ g \in \gl_{n+1}(V) \mid \Tr{g} = 0 \}
        \]
        As a vector space over $\F$ it has dimension $(n^2 + 2n)$.
        \item Let $V = \F^{2n}$. The \textbf{symplectic algebra} $C_n$ is
        \[
            C_n = \liealg{sp}_{2n}(V) = \{ g \in \gl_{2n}(V) \mid sg + g^T s = 0 \}, \quad \text{where } s = \begin{pmatrix} 0 & \Id_n \\ -\Id_n & 0 \end{pmatrix}
        \]
        Considering its dimension, writing also $g \in \gl_{2n}(V)$ as $n$-by-$n$ blocks, we have
        \[
            g = 
            \left(\begin{array}{c|c}
                g_1 & g_2 \\
                \hline
                g_3 & g_4
            \end{array}\right)
            \quad
            \implies
            \quad
            sg + g^T s = 
            \left(\begin{array}{c|c}
                g_3 - g_3^T & g_4 + g_1^T \\
                \hline
                g_1 + g_4^T & g_2 - g_2^T
            \end{array}\right)
            = 0
        \]
        Further notice that $(g_4 + g_1^T) = (g_1 + g_4^T)^T$. Then the condition of matrix vanishing becomes equivalent to $g_3 = g_3^T$, $g_2 = -g_2^T$ and $g_1 = -g_4^T$. For a symmetric matrix its dimension is $\frac{1}{2}n(n+1)$, and arbitrary $g_1$ fixes $g_4$, giving dimension $n$. Summing together gives the $\dim_\F C_n = 2n^2 + n$.
        \item Let $V = \F^{2n+1}$. The \textbf{(odd) orthogonal algebra} $B_n$ is
        \[
            B_n = \liealg{o}_{2n+1}(V) = \{ g \in \gl_{2n+1}(V) \mid sg + g^T s = 0 \}, \quad \text{where } s = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & \Id_n \\ 0 & \Id_n & 0 \end{pmatrix}
        \]
        Considering its dimension, write also $g \in \gl_{2n+1}(V)$ in block form:
        \[
            g = 
            \begin{pmatrix}
                x              & a_{1 \times n} & b_{1 \times n} \\
                c_{n \times 1} & m_{n \times n} & n_{n \times n} \\
                d_{n \times 1} & p_{n \times n} & q_{n \times n}
            \end{pmatrix}
            \quad
            \implies
            \quad
            sg + g^T s =
            \begin{pmatrix}
                2x      & a + d^T & b + c^T \\
                a^T + d & p + p^T & q + m^T \\
                b^T + c & m + q^T & n + n^T
            \end{pmatrix}
            = 0
        \]
        This translates to equalities
        \[
            2x = 0, \quad
            a + d^T = (a^T + d)^T = 0, \quad
            b + c^T = (b^T + c)^T = 0, \quad
            q + m^T = (m + q^T)^T = 0, \quad 
            p + p^T = 0, \quad
            n + n^T = 0
        \]
        This gives $x = 0$. Fixing $a$ and $b$ fixes $d$ and $c$, of which there are $2n$ choices. Fixing $m$ fixes $q$, of which there are $n^2$ choices. It is also required that both $n$ and $p$ are anti-symmetric matrices, i.e. they have zeros on the diagonal, and fixing upper triangle elements fixes the whole matrix, of which there are $\frac{1}{2}n(n-1)$ choices. Then
        \[
            \dim_\F B_n = 2n + n^2 + 2 \cdot \frac{1}{2} n(n-1) = 2n^2 + n
        \]
        \item Let $V = \F^{2n}$. The \textbf{(even) orthogonal algebra} $D_n$ is
        \[
            D_n = \liealg{o}_{2n}(V) = \{ g \in \gl_{2n}(V) \mid sg + g^T s = 0 \}, \quad \text{where } s = \begin{pmatrix} 0 & \Id_n \\ \Id_n & 0 \end{pmatrix}
        \]
        Considering its dimension, use the similar strategy as above. Write $g \in \gl_{2n}(V)$ in block form:
        \[
            g = 
            \begin{pmatrix}
                m & n \\
                p & q
            \end{pmatrix}
            \quad \implies \quad
            sg + g^T s =
            \begin{pmatrix}
                p + p^T & q + m^T \\
                m + q^T & n + n^T
            \end{pmatrix}
            = 0
        \]
        The equality translates to the following conditions
        \[
            p + p^T = 0, \quad
            q + m^T = (m + q^T)^T = 0, \quad
            n + n^T = 0
        \]
        Fixing $p$ fixes $m$, and both $p$ and $n$ are anti-symmetric matrices, which have dimension $\frac{1}{2}n(n-1)$ over $\F$. This gives
        \[
            \dim_\F D_n = n^2 + 2 \cdot \frac{1}{2}n(n-1) = 2n^2 - n
        \] 
    \end{enumerate}
    \ 
\end{example}

\section{Analogy with other Algebraic Structures}

\textstart
Now we turn to discuss the algebraic structures of Lie algebra. Being an algebra itself, we are interested in some special properties, analogous to the study of groups or rings. In general, some of the definitions and theorems can be rephrased in a categorical sense, and thereby applies to all such similar structures.

\begin{definition}[$\F$-Algebra]
    Let $\F$ be a field. An $\F$-algebra is a vector space $U$ over $\F$ equipped with a bilinear map $(\cdot): U \times U \to U$, denoted $(u_1, u_2) \mapsto u_1 u_2$.
\end{definition}
\nogap
\begin{remark}
    In this sense, the Lie algebra $L$ (by definition is an $\F$-vector space) is indeed an $\F$-algebra (compatible with its name), with the bilinear map $(\cdot)$ being the Lie bracket.
\end{remark}

\begin{definition}[Derivation]
    Let $U$ be a vector space over field $\F$. A \textbf{derivation} of $U$ is $S \in \gl(U)$ s.t. for all $a, b \in U$, $S(ab) = a S(b) + S(a) b$ (i.e. satisfies the \underline{Leibniz Rule}). The set of derivations on $U$ is denoted by $\Der(U)$.
\end{definition}

\begin{remark}
    $\Der(U)$ is a subalgebra of $\gl(U)$. By definition, it suffices to verify that for all $S, T \in \Der(U)$, $[S, T] \in \Der(U)$. Check that the Leibniz rule is satisfied:
    \begin{align*}
        [S, T](ab) &= S(T(ab)) - T(S(ab)) \\
        & = S(aT(b) + T(a)b) - T(aS(b) + S(a)b) \\
        & = aS(T(b)) + bS(T(a)) - aT(S(b)) - bT(S(a)) \\
        & = a[S, T](b) + b[S, T](a)
    \end{align*}
\end{remark}

\begin{definition}[Adjoint Representation]
    The map $\ad: L \to \Der(L)$ is the \textbf{adjoint representation}, defined by $\ad(x)(y) = [x, y]$
\end{definition}

\textstart
An immediate thought is to verify that indeed $\ad(x)$ gives a derivation. Recall that the product defined in Lie algebra is the Lie bracket. Therefore, to verify the derivation property it suffices to check whether we have the equality
\[
    \ad(x)([y, z]) = [\ad(x)(y), z] + [y, \ad(x)(z)]
\]
Applying the definition and manipulating the terms, we get the Jacobi Identity, which verifies the equality:
\begin{align*}
    \ad(x)([y, z])
    & = [x, [y, z]] = -[y, [z, x]] - [z, [x, y]] = -[y, \ad(x)(z)] - [z, \ad(x)(y)] \\
    & = [\ad(x)(y), z] + [y, \ad(x)(z)]
\end{align*}

\begin{definition}[Structural Constants]
    Let $\{x_1, \dots, x_n\}$ be a basis of $L$ as an $\F$-vector space. The \textbf{structural constants} of $L$ are the coefficients $c_{ij}^k$ s.t. $[x_i, x_j] = \sum_{k = 1}^n c_{ij}^k x_k$.
\end{definition}

\textstart
It is clear that the structural constants are specified solely by the definition of Lie bracket, which is the sole extra structure given to any Lie algebra apart from the underlying vector space structure.

\begin{remark}
    Using the structural constants we can rewrite the Jacobi Identity. Given a basis of $L$ over $\F$ and its corresponding structural constants $a_{ij}^k \in \F$, the Jacobi Identity can be written as
    \[
        \sum_{k = 1}^n \left( a_{ij}^k a_{k \ell}^m + a_{j\ell}^k a_{ki}^m + a_{\ell i}^k a_{kj}^m \right) = 0, \quad \forall i, j, \ell, m \in \{1, \dots, n\}
    \]
\end{remark}

\begin{definition}[Abelian]
    A Lie algebra $L$ is \textbf{abelian} if $[x, y] = 0$ for all $x, y \in L$.
\end{definition}

\begin{definition}[Ideal]
    Given a Lie algebra $L$, a subspace $I \subseteq L$ is an \textbf{ideal} if for all $x \in I$ and $y \in L$, $[x, y] \in I$.
\end{definition}
\nogap
\begin{remark}
    This kind of ``absorbing'' property is analogous to the normal subgroup in group theory. Considering the Lie bracket as a multiplication on a ring, this is compatible with the definition of an ideal in a ring.
\end{remark}
\nogap
\begin{remark}\label{rmk: extending ideals}
    Given a Lie algebra $L$, if both $I$ and $J$ are ideals in $L$, then $I + J$, $I \cap J$ and $[I, J]$ are also ideals.
\end{remark}

\textstart
With the similar formulation of structure preserving sub-objects, we have similar structures as in group or ring theory.

\begin{definition}[Quotient]
    Given a Lie algebra $L$ and an ideal $I \subseteq L$, the \textbf{quotient} $L/I$ is the vector space $L/I = \{ x + I \mid x \in L \}$ with the Lie bracket $[x + I, y + I] = [x, y] + I$.
\end{definition}
\nogap
\begin{definition}[Center]
    Given a Lie algebra $L$, the \textbf{center} of $L$ is defined as $Z(L) = \{ z \in L \mid [z, x] = 0 \text{ for all } x \in L \}$.
\end{definition}
\nogap
\begin{definition}[Derived Algebra]
    Given a Lie algebra $L$, the \textbf{derived algebra} of $L$ is $[L, L] = L' = L^{(1)}$, where $[L, L]$ can be interpreted as the set $[L, L] := \{ [x, y] \mid x, y \in L \}$
    By definition this is an ideal of $L$.
\end{definition}
\nogap
\begin{definition}[Simple]
    A Lie algebra $L$ is \textbf{simple} if $[L, L] \neq 0$ (i.e. it is non-trivial), and the only ideals of $L$ are trivial ($0$ and $L$).
\end{definition}

\begin{example}\label{ex: simple lie algebra}
    Let $L = \gl_n(\F)$. Then $[e_{ij}, e_{k\ell}] = \delta_{jk} e_{i \ell} - \delta_{\ell i} e_{kj}$. Setting $k = j$ and $i = \ell$ gives $\Tr([e_{ij}, e_{k\ell}]) = 0$. By definitions in Example \ref{ex: classical Lie algebra}, $L^{(1)} = [L, L] \simeq \liealg{sl}_n(F)$. Since specifically $\liealg{sl}_n(\F) \subseteq \gl_n(\F)$ which gives an ideal, $\gl_n(\F)$ is not simple. In fact $\liealg{sl}_n(\F)$ is simple, but proving this result requires more constructions.
\end{example}

\begin{definition}[Normalizer]
    Let $L$ be a Lie algebra and $K$ a subalgebra of $L$. The \textbf{normalizer} of $K$ is defined by $N_L(K) := \{ x \in L \mid [x, K] \subseteq K \}$, the largest subalgebra of $L$ in which $K$ is an ideal.
\end{definition}
\nogap
\begin{definition}[Centralizer]
    Let $L$ be a Lie algebra and $X$ an arbitrary set. The \textbf{centralizer} of $X$ is defined by $C_L(X) := \{ x \in L \mid [x, X] = 0 \}$.
\end{definition}

\begin{definition}[Morphism]
    Let $L$ and $L'$ be two Lie algebras over $\F$. A \textbf{homomorphism} is an $\F$-linear transformation $\phi: L \to L'$ s.t. $\phi([x, y]) = [\phi(x), \phi(y)]$, i.e. it commutes with the multiplication, which is the Lie bracket here.

    Adopting general categorical nomenclature, a homomorphism is a \textbf{monomorphism} if it is injective, an \textbf{epimorphism} if it is surjective, an \textbf{isomorphism} if it is bijective, and an \textbf{automorphism} if it is an isomorphism from $L$ to itself.
\end{definition}
\nogap
\begin{remark}\label{rmk: extending morphism results}
    With the definition of morphisms, we can easily translate common results from group or ring theory to Lie algebras. For all homomorphism $\phi: L \to L'$:
    \begin{enumerate}
        \item Both $\ker \phi$ and $\im \phi$ are ideals of $L$.
        \item (First Isomorphism Theorem) $\im \phi \simeq L/\ker \phi$.
        \item (Second Isomorphism Theorem) Given two ideals $I, J \subseteq L$, by Remark \ref{rmk: extending ideals} $I + J$ is also an ideal of $L$. We have the isomorphism $(I + J)/J \simeq I/I \cap J$.
        \item (Third Isomorphism Theorem) Let $I \subseteq J \subseteq L$ where both $I$ and $J$ are ideals of $L$. Then $L/J \simeq (L/I)/(J/I)$.
        \item (Fourth Isomorphism Theorem, Correspondence) Let $I \subseteq \ker \phi$ be an ideal in $\ker \phi$. Then there exists a unique homomorphism $\psi: L/I \to L'$, i.e. making the following diagram commute:
        
        \begin{minipage}{\linewidth}
            \centering
            \begin{tikzcd}
                L \arrow[rr, "\phi"] \arrow[rrdd] & & L' \\
                & & \\
                & & L/I \arrow[uu, dashed, "\psi"]
            \end{tikzcd}
        \end{minipage}
    \end{enumerate}
    \ 
\end{remark}

\begin{definition}[Representation]
    Let $L$ and $L'$ be Lie algebras over $\F$, and $V$ a vector space over $\F$. A \textbf{representation} of $L$ on $V$ is a homomorphism $\rho: L \to \gl(V)$.
\end{definition}

\begin{example}[Adjoint Representation]
    For a Lie algebra $L$, recall that the adjoint map is $\ad: L \to \gl(L)$, $\ad(x)(y) = [x, y]$. Since $L$ is itself a vector space over $\F$, letting $V = L$ we have a representation. This is indeed a homomorphism as
    \begin{align*}
        [\ad(x), \ad(y)](z) 
        & = \ad(x) \ad(y)(z) - \ad(y) \ad(x)(z) = [x, [y, z]] - [y, [x, z]] \\
        & = [x, [y, z]] + [y, [z, x]] = -[z, [x, y]] = [[x, y], z] \\
        & = \ad([x, y])(z)
    \end{align*}
\end{example}

\begin{proposition}
    Any simple Lie algebra is isomorphic to a linear Lie algebra.
\end{proposition}

\begin{proof}
    Consider using the adjoint representation. The kernel of $\ad$
    \[
        \ker(\ad) = \{ x \in L \mid \ad(x) = 0 \} = \{ x \in L \mid [x, z] = 0, \forall z \in L \} = Z(L)
    \]
    by Remark \ref{rmk: extending morphism results} is an ideal in $L$. Since $L$ is simple, $\ker(\ad) = 0$. Using \hyperref[rmk: extending morphism results]{First Isomorphism Theorem}, $L/\ker(\ad) \simeq L \simeq \im(\ad)$. Since $\ad: L \to \gl(V)$, $\im(\ad) \subseteq \gl(L)$. By Definition \ref{def: linear lie algebra} $L$ is a linear Lie algebra.
\end{proof}

\begin{notation}
    It is often denoted $\GL(V) = \{ g \in \gl(V) \mid g \text{ invertible} \}$. This coincides with the general linear group.
\end{notation}
\nogap
\begin{example}[Adjoint Representation of $\GL(V)$]
    Let $g \in \GL(V)$. Define $\Ad(g): \gl(V) \to \gl(V)$ by $\Ad(g)(x) = gxg^{-1}$, which is the adjoint representation of $\GL(V)$.
\end{example}

\begin{definition}[Nilpotent (Element)]
    Assume that $\fchar{\F} = 0$. Let $L$ be a Lie algebra on $\F$. Then $x \in L$ is \textbf{nilpotent} if $(\ad(x))^k = 0$ for some $k \in \Z_{\geq 0}$.
\end{definition}

\begin{definition}[Exponential Map]
    Let $L$ be a Lie algebra, and $x \in L$ nilpotent of order $k$. Then the \textbf{exponential map} is defined by
    \[
        \exp(\ad(x)) = \sum_{n = 0}^{k-1} \frac{(\ad(x))^n}{n!}
    \] 
    which coincides with the Taylor expansion. ALl terms of order $k$ vanishes due to $x$ being $k$-nilpotent.
\end{definition}

\begin{lemma}
    $\exp(\ad(x)) \in \Aut(L)$. More generally, if $\delta \in \Der(L)$ is nilpotent, then $exp(\delta) \in \Aut(L)$.
\end{lemma}

\begin{proof}
    We seek to prove only the general version of the statement. First verify that $\ad$ is a derivation, i.e. we have the equality $\ad([x, y])(z) = ([x, \ad(y)] + [\ad(x), y])(z)$ for all $x, y, z \in L$. This is a direct consequence of the Jacobi Identity:
    \[
        \ad([x, y])(z) = [[x, y], z] = [x, [y, z]] - [y, [z, x]] = [x, \ad(y)(z)] - [y, \ad(x)(z)] = ([x, \ad(y)] + [\ad(x), y])(z)
    \]
    Now for the general statement on derivations, first verify that $\delta$ is a homomorphism. Write out the expressions explicitly:
    \begin{align*}
        \exp(\delta(x)) \exp(\delta(y))
        & = \left( \sum_{n = 0}^{k-1} \frac{\delta^n(x)}{n!} \right) \left( \sum_{m = 0}^{k-1} \frac{\delta^m(y)}{m!} \right) \\
        & = \sum_{n = 0}^{2k-2} \left( \sum_{i = 0}^n \frac{\delta^i(x)}{i!} \cdot \frac{\delta^{n-i}(y)}{(n-i)!} \right) \\
        & = \sum_{n = 0}^{2k-2} \frac{\delta^n(xy)}{n!} & \text{(Leibniz Rule)} \\
        & = \sum_{n = 0}^{k-1} \frac{\delta^n(xy)}{n!} = \exp(\delta(xy)) & \text{($\delta^k = 0$)}
    \end{align*}
    The fact that $\exp(\delta)$ is a bijection can be verified via writing out its inverse: let $\exp(\eta) = \Id - \eta$. Then $\exp^{-1} = \sum_{n=0}^{k-1} \eta^n$, where we designate $\eta^0 = \Id$. 
\end{proof}

\begin{example}
    Let $\F$ be a field of characteristic $0$, and $L = \F^2$ with basis $\{v_1, v_2\}$. Endow it with a Lie algebra structure by setting $[v_1, v_2] = v_1$. Then $\ad \in \gl_2(L)$, with the matrix representation
    \[
        \ad(v_1) = 
        \left(
        \begin{array}{c|c}
            \ad(v_1)(v_1) & \ad(v_1)(v_2) \\
        \end{array}
        \right)
        = 
        \begin{pmatrix}
            0 & 1 \\
            0 & 0
        \end{pmatrix}
        \quad
        \ad(v_1)^2 = 0
    \]
    Since $\ad$ is 2-nilpotent, 
    \[
        \exp(\ad(v_1)) = \Id_2 + 
        \begin{pmatrix}
            0 & 1 \\
            0 & 0
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 1 \\
            0 & 1
        \end{pmatrix}
    \]
    Using the same notation as in the proof above,
    \[
        \eta =
        \begin{pmatrix}
            0 & -1 \\
            0 & 0
        \end{pmatrix}
        \quad \implies \quad
        \exp^{-1}(\ad(v_1)) =
        \begin{pmatrix}
            1 & -1 \\
            0 & 1
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 1 \\
            0 & 1
        \end{pmatrix}^{-1}
    \]
    \ 
\end{example}

\section{Solvable and Nilpotent Lie Algebras}

\begin{definition}[Derived Series]\label{def: derived series}
    Given a Lie algebra $L$, the \textbf{derived series} of $L$ is the sequence of ideals
    \[
        L^{(0)} = L, \quad L^{(1)} = [L, L], \quad \cdots \quad L^{(n)} = [L^{(n-1)}, L^{(n-1)}]
    \]
\end{definition}
\nogap
\begin{remark}
    Similar to the cases in group or ring theory, the relation of ideals is in general not transitive. That is, directly from the expression we know that for all $n$, $L^{(i)}$ is an ideal in $L^{(i-1)}$; but in general $L^{(n)}$ is not an ideal in $L$ for $n \in \Z_{\geq 2}$.
\end{remark}
\nogap
\begin{definition}[Solvable]
    A Lie algebra $L$ is \textbf{solvable} if $L^{(n)} = 0$ for some $n$.
\end{definition}

\begin{example}\label{ex: derived series}
    Fix $\F$ to be a field. Let $\liealg{t}_n(\F)$ be upper triangular $n$-by-$n$ matrices over $\F$, $\liealg{d}_n(\F)$ be diagonal $n$-by-$n$ matrices and $\liealg{n}_n(\F)$ strictly upper triangular $n$-by-$n$ matrices. Then $\liealg{d}_n(\F)$ is solvable since in general diagonal matrices commutes with all matrices in terms of multiplication (i.e. lies in the center of the Lie algebra). $\liealg{n}_n(\F)$ is also solvable since strictly upper triangular matrices are nilpotent. Furthermore, as vector spaces $\liealg{t}_n(\F) = \liealg{d}_n(\F) \oplus \liealg{n}_n(\F)$, giving $\liealg{t}_n(\F)$ solvable.

    In general we can express the $k$-th derived algebra of $\liealg{t}_n(\F)$ as
    \[
        \liealg{t}_n(\F)^{(k)} = \{ (g_{ij}) \mid g_{ij} = 0 \forall i + k - 1 \geq j \}
    \]
    which is nilpotent of order $n-1$.
\end{example}
\nogap
\begin{example}
    All simple Lie algebras are non-solvable. By definition $[L, L]$ is an ideal in $L$; and $L$ being simple implies that $[L, L]$ must be either 0 or $L$. However being simple also requires that $L$ is not abelian. Therefore $L^{(1)} = [L, L] = L$, giving $L^{(k)} = L$ for all $k$ and therefore is non-solvable. One of such examples is $\liealg{sl}_n(\F)$ (also mentioned but not proved in \ref{ex: simple lie algebra}).
\end{example}

\begin{remark}\label{rmk: extending solvability}
    Similar to solvability in group theory, we have the following results:
    \begin{enumerate}
        \item If $L$ is solvable, then so are its subalgebras and $\im \phi$ for all $\phi: L \to L'$ homomorphisms.
        \item For $I \subseteq L$ an ideal, if both $I$ and $L/I$ are solvable, then $L$ is solvable.
        \item If $I$ and $J$ are solvable ideals in $L$, then $I + J$ is also solvable.   
    \end{enumerate}
\end{remark}

\begin{corollary}
    If $L$ is solvable, then $L$ has a unique solvable ideal, called the \textbf{radical ideal}, denoted $\Rad L$.
\end{corollary}

\begin{proof}
    Let $S$ be a maximal solvable ideal of $L$, and $I$ any solvable ideal of $L$. There fore $S \subseteq I + S$, and by Remark \ref{rmk: extending solvability} $I + S$ is solvable. Maximality of $S$ implies $I + S = S$, and therefore $I \subseteq S$ which verifies the uniqueness. Existence follows from the fact that the zero ideal is solvable.
\end{proof}

\begin{definition}[Semisimple]
    A non-zero Lie algebra $L$ is \textbf{solvable} if $\Rad(L) = 0$.
\end{definition}

\begin{remark}
    We have the following properties directly resulting from the definition:
    \begin{enumerate}
        \item A simple Lie algebra is semisimple (as expected), as the radical ideal is an ideal.
        \item If a Lie algebra $L$ is not solvable, then $L/\Rad L$ is semisimple, as by Correspondence Theorem a solvable ideal $J \subseteq L/\Rad L$ corresponds to a solvable ideal $I \subseteq L$ containing $\Rad L$. From definition of radical ideals $I = J$, and therefore $L/\Rad L$ is semisimple.
    \end{enumerate}
\end{remark}

\begin{definition}[Descending (Lower) Central Series]
    Given a Lie algebra $L$, its \textbf{descending central series} (or \textbf{lower central series}) is the sequence of ideals
    \[
        L^{0} = L, \quad L^{1} = [L, L], \quad \cdots \quad L^{n} = [L, L^{n-1}]
    \]
    The notation is consistent with the treatment of Lie brackets in Lie algebras as multiplication.
\end{definition}
\nogap
\begin{remark}
    Different from the \hyperref{def: derived series}{derived series}, every term in the descending central series is an ideal in $L$.
\end{remark}

\textstart
With the notion of ``raising a power'' for Lie algebras, we can extend the definition of nilpotency:

\begin{definition}[Nilpotent (Lie Algebra)]
    A Lie algebra $L$ is \textbf{nilpotent} if $L^{n} = 0$ for some $n$.
\end{definition}
\nogap
\begin{example}
    Revisiting the example for derived series (Example \ref{ex: derived series}) with the same notation, although similarly both $\liealg{n}_n(\F)$ and $\liealg{d}_n(\F)$ are nilpotent, $\liealg{t}_n(\F)$ is not nilpotent, as one can always fill in a diagonal matrix for $L$. The explicit expression for $\liealg{n}_n(\F)$ agrees the nilpotency of upper triangular matrices:
    \[
        \liealg{n}_n(\F)^k = \{ (g_{ij}) \mid g_{ij} = 0 \forall i + k \geq j \}
    \]
\end{example}

\begin{remark}
    Similarly the properties of nilpotent groups extend to nilpotent Lie algebras:
    \begin{enumerate}
        \item If a Lie algebra $L$ is nilpotent, and all its subalgebras and $\im \phi$ for all $\phi: L \to L'$ homomorphism are nilpotent. 
        \item If $L/Z(L)$ is nilpotent, then $L$ is nilpotent.
        \item If $L \neq 0$ is nilpotent, then $Z(L) \neq 0$.
    \end{enumerate}
\end{remark}

\begin{definition}[Ad-nilpotent]
    Given a Lie algebra $L$, an element $x \in L$ is \textbf{ad-nilpotent} if $\ad(x)$ is nilpotent.
\end{definition}

\begin{lemma}
    If $L$ is nilpotent, then for all $x \in L$, $x$ is ad-nilpotent.
\end{lemma}

\begin{proof}
    Since $L$ is nilpotent, there exists $n$ s.t. $L^n = 0$. Since $\ad(x)^n (y) \in L^n$, $\ad(x)^n = 0$.
\end{proof}

\textstart
We now seek prove the converse. From now on, assume that $\F$ is a field of characteristic and algebraically closed. Our main focus will be Lie algebras over such fields. For simplicity, when referring to the dimension of a Lie algebra, we will simply use $\dim (-)$ instead of $\dim_{\F} (-)$.

\begin{theorem}[Engel]\label{thm: Engel}
    Let $L$ be a Lie algebra. If for all $x \in L$, $x$ is ad-nilpotent, then $L$ is nilpotent.
\end{theorem}

\begin{lemma}
    If $x \in \gl(V)$ is a nilpotent endomorphism, then $\ad(x)$ is nilpotent.
\end{lemma}

\begin{proof}
    This can be verified via writing out the expression explicitly. Using induction, we have
    \[
        \ad^n(x) (y) = \sum_{i = 0}^n \binom{n}{i} x^i y x^{n-i}
    \]
    Suppose that $x$ is $k$-nilpotent. Choosing $n = 2k$ makes $\ad^n(x)$ vanish.
\end{proof}

To prove Engel's theorem, we first prove the following theorem:

\begin{theorem}\label{thm: 0 is eigenvalue of lie algebra of nilpotent endomorphisms}
    Let $L$ be a subalgebra of $\gl(V)$, for any vector space $V$ s.t. $\dim V$ is finite. If $L$ consists of nilpotent endomorphisms and $V \neq 0$, then there exists a nonzero $v \in V$ s.t. $L \cdot v = 0$, i.e. 0 is always an eigenvalue of $L$.
\end{theorem}
\nogap
\begin{remark}
    This is the analogy of a similar (yet under that context much easier to prove) result in linear algebra: given a homomorphism that is not surjective, the kernel is non-trivial; and therefore the dimension of the image is strictly less than the dimension of the domain.
\end{remark}

\begin{proof}
    Apply induction on the dimension of $L$:
    \begin{itemize}
        \item $\dim L = 0$. Then any nonzero $v \in V$ works (the result is vacuous).
        \item $\dim L = 1$. Then $L$ can be written as the linear span of a single element $L = \linspan{\ell}$. $\ell$ being nilpotent implies that its only eigenvalue is 0. Therefore for any $v$ being an eigenvector of $L$, $\ell v = 0 \implies L \cdot v = 0$.
        \item \emph{Inductive Step.} Let $K \subsetneq L$ be a maximal subalgebra. (This can be 0, which degenerates to the case of $\dim L = 0$) $K \neq L$ implies that $\dim K < \dim L$, giving $\dim (L/K) < \dim L$. Now take $V = L/K$, and apply the inductive hypothesis: there exists a nonzero $v + K \in L/K$ s.t. $(L/K) \cdot (v + K) = 0$, i.e. $L\cdot v \in K$. Recall that the action of Lie algebra on itself is via adjoint representation, i.e. we have $[y, x] \in K$ for all $y \in K$ and $x \in L$. Since $K$ is maximal in $L$, $\dim K = \dim L - 1$. Therefore, we can write $L = \linspan{K, x}$. By the inductive hypothesis, there exists a nonzero $v \in V$ s.t. $K \cdot v = 0$. Denote all such elements by $W := \{ v \in V \mid K\cdot v = 0 \}$. We now seek to prove that this set is also annihilated by $L$. 
        
        First observe that for all $k \in K, w \in W$, $kxw = x(kw) - [x, k]w$. Inductive hypothesis on $K$ implies that $kw = 0$ and therefore $x(kw) = 0$; and inductive hypothesis on $L/K$ implies that $[x, k]w = 0$ for all $x \in L$. By definition of $W$, this gives $x \cdot w \in W$, i.e. $\linspan{x}$ is a subalgebra of $\gl(W)$. Since $\linspan{x}$ is 1-dimensional, IH implies that there exists $w_0 \in W$ s.t. $x \cdot w_0 = 0$. Then decomposing application of $L$ on $w_0$ gives $L \cdot w_0 = K \cdot w_0 + \F x \cdot w_0 = 0$.
    \end{itemize}
\end{proof}

\textstart
Using this theorem it is then straightforward to prove Engel's theorem:

\begin{proof}[Proof of Theorem \ref{thm: Engel}]
    Similarly apply induction on the dimension of $L$:
    \begin{itemize}
        \item $\dim L = 1$. In this case, $L$ is abelian, i.e. $L^2 = [L, L] = 0$. By definition $L$ is nilpotent.
        \item \emph{Inductive step.} Suppose that the result holds for $V$ s.t. $\dim V \leq n$. By hypothesis, for all $x \in L$, $\ad(x)$ is nilpotent. Use the notation of Theorem \ref{thm: 0 is eigenvalue of lie algebra of nilpotent endomorphisms}, letting $V = L$ gives that $\ad(L)$ is a subalgebra of $\gl(L) = \gl(V)$. The result of Theorem \ref{thm: 0 is eigenvalue of lie algebra of nilpotent endomorphisms} gives that there exists $x \in L$ s.t. $\ad(L)(x) = [L, x] = 0$. This implies that $Z(L) \ni x$ is nonzero. Since $Z(L)$ is a vector subspace, $\dim (L/Z(L)) < \dim L$. Inductive hypothesis gives that under the hypothesis, $\dim (L/Z(L))$ is nilpotent. Furthermore, $Z(L)$ is nilpotent by definition (as $(Z(L))^2 = 0$). By Remark \ref{rmk: extending solvability}, $L$ is nilpotent.
    \end{itemize}
\end{proof}

\begin{definition}[Flag]
    Assume that $\dim V$ is finite. A \textbf{flag} in $V$ is a sequence of subspaces
    \[
        0 = V_0 \subseteq V_1 \subseteq \cdots \subseteq V_n = V \quad \text{s.t.} \quad \dim V_i = i
    \]
\end{definition}
\nogap
\begin{definition}[Stabilizer on Flags]
    An element $x \in \End(V)$ \textbf{stabilizes} a flag if $x(V_i) \subseteq V_i$ for all $i$.
\end{definition}

\textstart
Using ``flags'' we can describe the theorem above in a language more resembling that in vector spaces:

\begin{corollary}
    If $L \in \gl(V)$ consists of nilpotent endomorphisms, then there exists a flag $(V_i)$ stable under $L$, i.e. in such basis $L \in \liealg{n}_n(\F)$, which can be expressed by a strictly upper triangular matrix.
\end{corollary}

\begin{proof}
    It suffices to give a construction of such flag. Theorem \ref{thm: 0 is eigenvalue of lie algebra of nilpotent endomorphisms} implies that there exists nonzero $v \in V$ s.t. $L\cdot v = 0$. Set $V_1 = \F v$ and $W = V / V_1$. Repeat the process until $W = 0$; and ``flatten'' the quotient spaces by taking the preimage of the quotient map, and consider the elements as vector subspaces of $V$. 
\end{proof}

\begin{lemma}
    Let $L \neq 0$ be a nilpotent Lie algebra, and $I \subseteq L$ a nonzero ideal. Then $I \cap Z(L) \neq 0$.
\end{lemma}

\begin{proof}
    $L$ being nilpotent implies that the derived algebra $L' = [L, L] \subsetneq L$, i.e. $Z(L) \neq 0$. Consider again the action of $L$ on itself via adjoint. Apply again Theorem \ref{thm: 0 is eigenvalue of lie algebra of nilpotent endomorphisms} with $V = I$, there exists $x \in I$ s.t. $\ad(L)(x) = 0$, i.e. $x \in Z(L)$. This gives a nonzero element in $I \cap Z(L)$.
\end{proof}

\begin{theorem}\label{thm: solvable implies common eigenvector}
    Let $L$ be a solvable Lie subalgebra of $\gl(V)$, and assume that $\dim V$ is finite. If $V \neq 0$, then $V$ contains a common eigenvector (but the eigenvalue may change), i.e. there exists $v \in V$ s.t. for all $\ell \in L$, $\ell v = \lambda(\ell) v$ where $v$ is an eigenvector of $\ell$.
\end{theorem}

\begin{proof}
    Similar to the proof of the previous result, proceed via applying induction on $\dim L$. The case of $\dim L = 1$ is given by any scalar. 
    
    First sketch the framework of the proof:
    \begin{itemize}
        \item First define an ideal of codimension 1 in $L$. Using the inductive hypothesis to get a common eigenvector.
        \item Verify that $L$ stabilizes a certain space containing that eigenvector. 
        \item Find an eigenvector $z$ in that space s.t. $L + I + \F z$.
    \end{itemize}
    Now proceed the proof:
    \begin{itemize}
        \item \bu{Step 1}. Since $\dim L > 0$ and $L$ is solvable, $[L, L] \subsetneq L$ (otherwise the derived series will have identical terms and thus do not terminate). By definition $L/[L, L]$ is abelian.\footnote{This quotient is to ensure that there exists such a codimension-1 subspace.} Then any subspace $I \subseteq L/[L, L]$ will be an ideal. Take $I' \subseteq L/[L, L]$ be an ideal of codimension 1. Take its preimage $I \subseteq L$ of the quotient map, which is an ideal of codimension 1 in $L$. Inductive hypothesis implies that there exists nonzero $v \in V$ s.t. for all $x \in I$, $x \cdot v = \lambda(x)v$. Fix $\lambda$ and define $W := \{ w \in V \mid x \cdot w = \lambda w, \forall x \in I \}$. Then $v \in V$ implies that $W \neq 0$, which is a nontrivial space fixed by $I$.
        \item \bu{Step 2}. We now want to show that the space is actually fixed by $L \supset I$. Use the same trick as the one in the previous proof: Let $w \in W, x \in L$; by definition, for all $y \in I$ we have the equality
        \begin{equation}\tag{$\ast$}\label{eq: transforming L, I application}
            (yx) w = xyw - [x, y]w = \lambda(y) xw - \lambda([x, y]) w
        \end{equation}
        To show that $xw \in W$, it suffices to show that $y xw = \lambda(y) xw$, i.e. $\lambda([x, y]) = 0$. Let $n > 0$ be the smallest integer s.t. $\{ w, xw, \cdots, x^{n-1}w \}$ are linearly independent. Define $W_i = \linspan{w, xw, \cdots, x^{i-1}w}$. It is clear that there are finitely many distinct $W_i$s, as $W_{n+j} = W_n$ for all $j \in \Z_{\geq 0}$; and $x W_n \subseteq W_n$. Eq. \eqref{eq: transforming L, I application} implies that $y x^i w = \lambda(y) x^i w$ for all $i$, i.e. $I W_n \subseteq W_n$. That is, $y$ is an upper triangular matrix w.r.t. the basis $\{ w, xw \cdots, x^{n-1}w \}$; and diagonal entries are all $\lambda(y)$, i.e. for all $y \in I, \Tr_{W_n}(y) = n \lambda(y)$. Since $I$ is an ideal, $y \in I \implies [x, y] \in I$, i.e. $\Tr_{W_n}([x, y]) = \lambda([x, y])$. However, $\Tr_{W_n}([x, y]) = 0$ as $\Tr_{W_n}(xy) = \Tr_{W_n}(yx)$ for al $x, y \in L$. This thus gives the desired result $\lambda([x, y]) = 0$.
        \item \bu{Step 3}. Since $I$ is codimension-1, we can write $L = I + \F z$ for some $z \in L$. Since $\F$ is assumed to be algebraically closed, there exists eigenvector $v_0 \in W$ s.t. $z v_0 = c v_0$ for all $z \in L$ with the corresponding $c \in \F$. Furthermore by definition of $W$, for all $y \in I$, $y \cdot v_0 = \lambda(y) v_0$, i.e. $v_0$ is a common eigenvector of $L$. 
    \end{itemize}
\end{proof}

\textstart
With the above result, we can state the following theorem (as a corollary):

\begin{theorem}[Lie]
    Let $L$ be a solvable Lie subalgebra of $\gl(V)$ with $\dim V$ finite. Then $L$ stabilizes a flag of $V$, i.e. there exists a basis of $V$ for which $L$ can be represented as an upper triangular matrix.
\end{theorem}

\textstart
The proof can be done by reiterating the \bu{Step 2} in the proof of Theorem \ref{thm: solvable implies common eigenvector} above.

\end{document}
